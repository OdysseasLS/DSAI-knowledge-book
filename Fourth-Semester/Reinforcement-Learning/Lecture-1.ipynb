{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning (RL) - Lecture 1**\n",
    "\n",
    "- **Reinforcement Learning is inspired by Instrumental Conditioning (Skinner Box)**\n",
    "\n",
    "| Feature | Supervised Learning | Reinforcement Learning |\n",
    "|---------|---------------------|------------------------|\n",
    "| Dataset | Given | Actively collected |\n",
    "| Feedback | Full (x with correct y) | Partial (state-action feedback) |\n",
    "| Goal | Learn a function from labeled data | Learn an **optimal policy** through interaction |\n",
    "\n",
    "---\n",
    "\n",
    "### **Chapter 1 Summary & Exam Notes**\n",
    "\n",
    "#### **Summary**\n",
    "This chapter introduces *Reinforcement Learning (RL)* as a computational approach to learning from interaction. Unlike supervised learning, RL involves an agent that learns through *trial-and-error* and *delayed rewards*, making decisions to maximize long-term rewards. The key difference from other learning methods is the trade-off between *exploration* (trying new actions) and *exploitation* (using known successful actions).\n",
    "\n",
    "RL is framed as a *Markov Decision Process (MDP)*, emphasizing three core elements:\n",
    "1. **Sensation (State)** – Perceiving the environment.\n",
    "2. **Action** – Choosing actions to interact with the environment.\n",
    "3. **Goal (Reward)** – A numerical signal guiding behavior.\n",
    "\n",
    "Key elements in RL include:\n",
    "- **Policy**: The strategy for selecting actions.\n",
    "- **Reward Function**: Determines immediate desirability of states/actions.\n",
    "- **Value Function**: Measures long-term expected rewards.\n",
    "- **Model (optional)**: Predicts state transitions for planning.\n",
    "\n",
    "The chapter presents examples like playing chess, controlling a refinery, and making breakfast to illustrate how RL applies to real-world decision-making. The *tic-tac-toe example* demonstrates RL principles using value function estimation and temporal-difference learning.\n",
    "\n",
    "Historically, RL stems from both psychology (trial-and-error learning) and optimal control theory (dynamic programming). Modern RL integrates these fields, evolving into a key area bridging AI and engineering.\n",
    "\n",
    "---\n",
    "\n",
    "- **Reinforcement Learning (RL)**: Learning through interaction to maximize cumulative reward.\n",
    "- **Trial-and-Error Learning**: Discovering the best actions by experimenting.\n",
    "- **Delayed Reward**: Actions influence future rewards, requiring foresight.\n",
    "- **Exploration vs. Exploitation**: Balancing trying new actions and leveraging past knowledge.\n",
    "- **Markov Decision Process (MDP)**: The mathematical framework underlying RL.\n",
    "\n",
    "#### **Main Elements of RL**\n",
    "1. **Policy (π)** – Mapping states to actions (deterministic or stochastic).\n",
    "2. **Reward Function (R)** – Immediate feedback for an action.\n",
    "3. **Value Function (V)** – Expected future rewards from a state.\n",
    "4. **Model (optional)** – Predicts environment transitions.\n",
    "\n",
    "#### **Comparison with Other Learning Methods**\n",
    "| Feature              | Reinforcement Learning | Supervised Learning | Evolutionary Methods |\n",
    "|----------------------|----------------------|----------------------|----------------------|\n",
    "| **Learning Style**   | Trial-and-error       | Given labeled data   | Direct policy search |\n",
    "| **Feedback Type**    | Reward signals        | Correct labels       | Population fitness   |\n",
    "| **Exploration Need** | Yes                   | No                   | Yes                  |\n",
    "\n",
    "#### **Important Challenges**\n",
    "- **Exploration-Exploitation Dilemma**: Agent must balance trying new actions vs. using known rewards.\n",
    "- **Credit Assignment Problem**: Determining which past actions led to success.\n",
    "- **Curse of Dimensionality**: Large state spaces make learning difficult.\n",
    "\n",
    "#### **Tic-Tac-Toe Example**\n",
    "- Uses **value function learning** to estimate probabilities of winning.\n",
    "- Updates values based on **temporal-difference (TD) learning**.\n",
    "- Learns optimal strategy through self-play.\n",
    "\n",
    "#### **Historical Context**\n",
    "- **Thorndike’s Law of Effect (1911)**: Reinforcement strengthens associations.\n",
    "- **Bellman’s Dynamic Programming (1957)**: Key foundation for RL.\n",
    "- **Minsky, Samuel (1950s-60s)**: Early AI applications of RL.\n",
    "- **Watkins (1989)**: Introduced *Q-learning*, a major RL algorithm.\n",
    "- **Tesauro’s TD-Gammon (1992)**: Demonstrated RL’s success in complex games.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
