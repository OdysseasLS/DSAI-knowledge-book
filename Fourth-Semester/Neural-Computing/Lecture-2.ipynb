{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **1. Associative Memory**\n",
    "- Also called **content-addressable memory**.\n",
    "- Retrieves a stored memory based on similarity to an input pattern.\n",
    "- Works with **incomplete or noisy** data.\n",
    "- Example: A binary image can be represented using a **binary Hopfield Network**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. The Hopfield Network (HN)**\n",
    "- **Recurrent neural network** that stores & retrieves multiple memories.\n",
    "- Inspired by the **Ising model** (magnetic fields in physics).\n",
    "- Network converges to **energy minima**, which correspond to stored patterns (memories).\n",
    "\n",
    "#### **Key Properties**\n",
    "1. **Connectivity** – Fully connected network with weight matrix storing connection strengths.\n",
    "2. **Activation** – Computed based on input from other neurons & their connection weights.\n",
    "3. **Bipolar State** – Neurons take values **-1 or +1** (thresholded output).\n",
    "\n",
    "#### **Symmetric Weights**\n",
    "- Connections are **bidirectional** (if neuron $ i $ connects to $ j $ with weight $ w $, then $ j $ to $ i $ has the same weight).\n",
    "- Learning = Adjusting **weight matrix** to enable correct memory retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Visual Representations**\n",
    "- **Vector Representation**: 1D array of neuron states.\n",
    "- **Matrix Representation**: Similar to an image (pixel-like).\n",
    "- **Graph Representation**: Neurons as nodes, weighted edges for connections.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Formal Definition**\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  O_i (t+1) = g \\left( \\sum_{j} w_{ij} O_j (t) + b_i \\right)\n",
    "  $$\n",
    "  - $ g(x) $ can be **tanh** or **sigmoid**.\n",
    "  - **Associative memory** retrieves the closest stored pattern.\n",
    "\n",
    "- **Stable States**:\n",
    "  - System evolves towards a stable state (stored memory).\n",
    "  - Input space is divided into **basins of attraction**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Stability & Attractors**\n",
    "- **Stable Pattern Condition**:\n",
    "  $$\n",
    "  O_i = \\text{sgn} \\left( \\sum_{j} w_{ij} \\xi_j \\right) = \\xi_i\n",
    "  $$\n",
    "  - If weights are set as $ w_{ij} \\propto \\xi_i \\xi_j $, the network retrieves $ \\xi $.\n",
    "\n",
    "- **Attractors**:\n",
    "  - Initialized from **random input**, the network evolves to a stable state.\n",
    "  - If weights are correctly set, it converges to the stored pattern (or its reverse).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Example: Multiple Stored Patterns**\n",
    "- **HN of 4096 neurons** can store & recall **three 64x64 pixel images**.\n",
    "- The network **restores distorted patterns** to their original form.\n",
    "- **Capacity Limit**: Stores about **0.15N** patterns (where $ N $ = number of neurons).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Energy Function**\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  O_i (t+1) = g \\left( \\sum_{j} w_{ij} O_j (t) + b_i \\right)\n",
    "  $$\n",
    "- **Energy Minimization**:\n",
    "  - Stable states correspond to **local minima** of the energy function.\n",
    "  - Energy **monotonically decreases** with each update.\n",
    "  - **Gradient Descent** can be used to optimize weights."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
