{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction & Biological Background**\n",
    "### **The Neuron**\n",
    "\n",
    "**Soma (Cell Body)**: Contains nucleus and integrates inputs.\n",
    "\n",
    "### **Connections Between Neurons**\n",
    "- **Action potential flow**: Axon → Axon Terminal → Synapse → Dendrites\n",
    "- **Types of signals**:\n",
    "  - **Excitatory (EPSP)**: Increases likelihood of firing.\n",
    "  - **Inhibitory (IPSP)**: Decreases likelihood of firing.\n",
    "\n",
    "## **2. Neural Dynamics & Neurotransmission**\n",
    "### **Physical Properties of Neurons**\n",
    "- **Ion Channels** regulate membrane potential:\n",
    "  - **Depolarization**: Sodium (Na+) enters, making the inside less negative (Excitatory).\n",
    "  - **Hyperpolarization**: Potassium (K+) leaves, making the inside more negative (Inhibitory).\n",
    "- **Action Potential (Spike)**: A rapid electrical signal that carries information.\n",
    "\n",
    "### **Neural Dynamics**\n",
    "- **Absolute Refractory Period**: Minimum time between two spikes.\n",
    "- **Relative Refractory Period**: Harder but possible to trigger another spike.\n",
    "- **Spike Trains**: Sequences of action potentials representing information.\n",
    "\n",
    "## **3. Artificial Neural Networks (ANNs)**\n",
    "### **Artificial Neuron (McCulloch-Pitts Model)**\n",
    "- **Mathematical model**:\n",
    "  $$ O = sgn( \\sum w_i x_i ) $$\n",
    "  - **Weights**: Represent synaptic strength.\n",
    "  - **Excitation**: Positive weight-product.\n",
    "  - **Inhibition**: Negative weight-product.\n",
    "\n",
    "### **Perceptron**\n",
    "- **Definition**: Simplest neural model using a sign function as an activation function.\n",
    "- **Equation**:\n",
    "  $$ O = sgn(w^T x + b) $$\n",
    "- **Computational Power**:\n",
    "  - A **linear binary classifier** (cannot solve XOR problem).\n",
    "  - **Decision boundary**: $ w^T x + b = 0 $.\n",
    "  - **Improvements**: Multi-layer perceptron (MLP) adds complexity.\n",
    "\n",
    "## **4. Types of Neural Networks**\n",
    "### **Feedforward Neural Network (FNN)**\n",
    "- No cycles, information flows in one direction.\n",
    "- **Network structure**:\n",
    "  - **Fully Connected**: All neurons in a layer connect to all neurons in the next.\n",
    "  - **Randomly Connected**: Some connections are omitted.\n",
    "  - **Optimized Architecture**: Best structure found via learning.\n",
    "\n",
    "### **Recurrent Neural Network (RNN)**\n",
    "- **Contains cycles**, allowing information to persist over time.\n",
    "- **Used for**: Time-series prediction, language modeling.\n",
    "- **Difficult to train** due to long-term dependencies.\n",
    "\n",
    "### **Hopfield Network**\n",
    "- **Equation**:\n",
    "  $$ O_i(t+1) = sgn \\sum w_{ij} O_j (t) $$\n",
    "- **Associative memory**: Recalls stored patterns from noisy inputs.\n",
    "\n",
    "## **5. Training & Learning in Neural Networks**\n",
    "### **Hebbian Learning (Biologically Inspired)**\n",
    "- \"Neurons that fire together wire together.\"\n",
    "- **Strengthens** connections between frequently co-activated neurons.\n",
    "\n",
    "### **Optimization & Backpropagation**\n",
    "- **Gradient Descent**: Adjusts weights to minimize error.\n",
    "- **Backpropagation**: Computes weight updates using chain rule (used in deep learning).\n",
    "- **Practical Considerations**:\n",
    "  - Vanishing gradient problem → Use activation functions like ReLU.\n",
    "  - Overfitting → Use regularization and dropout.\n",
    "\n",
    "## **6. Advanced Neural Network Models**\n",
    "### **Convolutional Neural Networks (CNNs)**\n",
    "- Used for **computer vision** (e.g., image recognition).\n",
    "- **Structure**:\n",
    "  - **Convolutional layers**: Extract patterns.\n",
    "  - **Pooling layers**: Reduce dimensionality.\n",
    "  - **Fully connected layers**: Make final predictions.\n",
    "\n",
    "### **Generative Models**\n",
    "- **GANs (Generative Adversarial Networks)**: Two networks (Generator & Discriminator) compete to create realistic data.\n",
    "- **Autoencoders**: Learn compact representations of data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
