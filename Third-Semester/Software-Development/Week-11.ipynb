{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Ethical Issues in Data Science**\n",
    "Data Science raises many ethical questions when it comes to the use of data. Real-life controversial examples highlight the importance of ethical practices:\n",
    "\n",
    "#### **Examples of Ethical Issues**:\n",
    "1. **Pregnancy Prediction**:\n",
    "   - Retailers track customers' shopping habits (e.g., products purchased and shopping times).\n",
    "   - Data was used to target pregnant women with baby product coupons, which felt invasive.\n",
    "   - **Key Issue**: Lack of consent and overuse of personal data for targeted marketing.\n",
    "\n",
    "2. **Allstate Telemetry Packages**:\n",
    "   - Telematics devices track driving habits to adjust insurance premiums.\n",
    "   - **Ethical Concerns**:\n",
    "     - Penalizing drivers based on GPS data (e.g., unsafe roads in poor areas).\n",
    "     - Data use for non-transparent purposes, leading to unfair financial outcomes.\n",
    "\n",
    "3. **OkCupid Data Scrape**:\n",
    "   - Researchers scraped 70,000 dating profiles and published identifiable data (age, gender, orientation).\n",
    "   - **Key Concern**: Even public data requires consent before it is reused or published.\n",
    "\n",
    "4. **Credit Scores**:\n",
    "   - Algorithms pull behavioral data (online/offline) for scoring without transparency.\n",
    "   - **Concerns**:\n",
    "     - Bias based on social media, ethnicity, or gender.\n",
    "     - Inaccurate data affecting financial opportunities.\n",
    "\n",
    "5. **AI Beauty Contest**:\n",
    "   - AI judged a beauty contest but showed racial bias due to non-diverse training data.\n",
    "   - **Lesson**: Training data must reflect diverse populations to prevent discrimination.\n",
    "\n",
    "6. **Face Detection Algorithms**:\n",
    "   - Algorithms failed to detect darker skin tones due to biased training datasets.\n",
    "   - **Impact**: Exclusion of certain groups, often unintentionally, highlights the need for representative data.\n",
    "\n",
    "7. **COMPAS Algorithm**:\n",
    "   - Used to predict recidivism (likelihood of reoffending) in criminal cases.\n",
    "   - **Bias**: African-American defendants were unfairly scored higher than white counterparts.\n",
    "\n",
    "8. **Amazon Hiring Tool**:\n",
    "   - A hiring tool displayed bias against women due to historical data favoring male applicants.\n",
    "   - **Key Issue**: Algorithms often amplify historical biases in training data.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. The 5 Cs of Data Ethics**\n",
    "These principles guide ethical practices in data science:\n",
    "\n",
    "1. **Consent**:\n",
    "   - Users should know and agree to how their data is used.\n",
    "   - Clarity is essential to ensure informed decisions.\n",
    "\n",
    "2. **Clarity**:\n",
    "   - Explain terms and conditions in simple, understandable language.\n",
    "   - Avoid lengthy, complex legal jargon.\n",
    "\n",
    "3. **Consistency**:\n",
    "   - Maintain ethical and fair practices consistently.\n",
    "   - Build user trust by adhering to policies.\n",
    "\n",
    "4. **Control**:\n",
    "   - Users should have control over their data, including options to delete it.\n",
    "\n",
    "5. **Consequences**:\n",
    "   - Consider potential consequences, including unintended or harmful outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Code of Conduct for Data Scientists**\n",
    "Ethical guidelines for data scientists include the following:\n",
    "\n",
    "#### **Key Principles**:\n",
    "1. **Observe Regulations**:\n",
    "   - Understand and comply with relevant data protection laws.\n",
    "   - Know why regulations exist and what they protect.\n",
    "\n",
    "2. **Respect Privacy**:\n",
    "   - Ensure personal identifiers (e.g., emails, IDs) remain private and anonymized.\n",
    "\n",
    "3. **Eliminate Bias**:\n",
    "   - Use diverse and representative data.\n",
    "   - Test for bias and error rates among different groups.\n",
    "\n",
    "4. **Avoid Fabrication or Falsification**:\n",
    "   - Report only genuine results without manipulating data.\n",
    "\n",
    "5. **Show Transparency**:\n",
    "   - Be open about data collection and analysis methods.\n",
    "   - Obtain informed consent from participants.\n",
    "\n",
    "6. **Secure Data Collection**:\n",
    "   - Use secure methods for storing and analyzing data.\n",
    "\n",
    "7. **Use Algorithms Responsibly**:\n",
    "   - Test algorithms for fairness and bias.\n",
    "   - Ensure they are explainable and ethical in use.\n",
    "\n",
    "8. **Consider Long-Term Impacts**:\n",
    "   - Evaluate societal implications of algorithms and data use.\n",
    "   - Avoid perpetuating inequality or privacy risks.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Algorithmic Fairness**\n",
    "#### **Definition**:\n",
    "Algorithms should operate without unjust bias, treating people fairly regardless of characteristics like age, gender, or race.\n",
    "\n",
    "#### **Types of Harm**:\n",
    "1. **Allocation Harm**: Unequal resource distribution (e.g., jobs or loans).\n",
    "2. **Quality-of-Service Harm**: Models work better for some groups than others (e.g., face detection algorithms).\n",
    "3. **Stereotyping**: Reinforces harmful or inaccurate stereotypes (e.g., biased search results).\n",
    "\n",
    "#### **Principles**:\n",
    "- **Individual Fairness**: Treat similar individuals similarly.\n",
    "- **Group Fairness**: Ensure equal treatment for different groups.\n",
    "\n",
    "---\n",
    "\n",
    "### **Six Ethical Issues (CNIL Framework)**:\n",
    "1. **Autonomous Machines**:\n",
    "   - Delegation of critical decisions to machines raises accountability concerns.\n",
    "   - Example: Responsibility for accidents by autonomous vehicles.\n",
    "\n",
    "2. **Bias, Discrimination, and Exclusion**:\n",
    "   - Algorithms can amplify systemic biases.\n",
    "   - **Solutions**:\n",
    "     - Use explainable and transparent algorithms.\n",
    "\n",
    "3. **Algorithmic Profiling**:\n",
    "   - Profiling can lead to misuse (e.g., Cambridge Analytica scandal in 2016 elections).\n",
    "\n",
    "4. **Massive Data Collection**:\n",
    "   - AI requires large datasets, but this must balance privacy concerns.\n",
    "   - Example: Non-identifiable datasets in health studies.\n",
    "\n",
    "5. **Data Quality and Bias**:\n",
    "   - Poor-quality training data can lead to harmful results.\n",
    "   - Example: Microsoft's Twitter bot (Tay) was manipulated into producing offensive content.\n",
    "\n",
    "6. **Human Identity and AI**:\n",
    "   - Human-machine hybridization raises ethical questions about emotional attachment to robots."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
