{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial General Intelligence :** reasoning, learning, problem solving, generalisation\n",
    "\n",
    "**Strong AI:** Machines that act intelligently can eventually also possess consciousness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm intelligence: agents interacting locally with one another and with the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical (Symbolic) AI or Good Old-fashioned AI (GOFAI)\n",
    "\n",
    "focusses on knowledge representation and general purpose “reasoning” mechanisms.\n",
    "\n",
    "- Separation between knowledge and reasoning.\n",
    "- Generic reasoning (problem-solving) based on search and logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Environment Properties:**\n",
    "- **Deterministic:** The next state is entirely predictable from the current state and action (e.g., a puzzle game like Tetris).\n",
    "- **Stochastic:** There is some randomness in the outcome of actions (e.g., Pacman, where ghosts can move unpredictably)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Agents:**\n",
    "\n",
    "1. **Simple Reflex Agents:**\n",
    "   - Act on current percepts using predefined condition–action rules.\n",
    "   - Limitations: Fail in partially observable environments.\n",
    "\n",
    "2. **Model-Based Reflex Agents:**\n",
    "   - Handle **partially observable environments** by maintaining an internal state.\n",
    "\n",
    "3. **Goal-based Agents:**\n",
    "4. **Utility-based Agents:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **Task Environments (PEAS Framework)**\n",
    "- **PEAS:** Performance measure, Environment, Actuators, Sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graph vs. Tree Search**\n",
    "- **Tree Search**: Considers all paths, including loops.\n",
    "- **Graph Search**: Avoids revisiting states by using an explored set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Uninformed Search**: No additional information about states beyond the problem definition.\n",
    "- **Informed Search**: Uses heuristics to estimate the quality of non-goal states (covered in Section 3.5).\n",
    "\n",
    "## Uninformed Search Methods\n",
    "\n",
    "Don’t have prior knowledge of the problem domain.\n",
    "\n",
    "- **Breadth-first search**\n",
    "- **Uniform-cost search**: Expands the node with the lowest path cost, $g(n)$, and is optimal for general step costs.\n",
    "Memory efficient, time efficient\n",
    "```bash\n",
    "    A\n",
    "   / \\\n",
    "  2   3\n",
    " /     \\\n",
    "B       C    \n",
    "|\\     /|\n",
    "4 1   2 5\n",
    "|  \\ /  |\n",
    "D   E   F\n",
    " \\  |  /\n",
    "  \\ | /\n",
    "   \\|/\n",
    "    G   \n",
    "```\n",
    "Steps:\n",
    "1. `(A, cost=0)`,              2. `[(B, cost=2), (C, cost=3)]`,             3. `[(C, cost=3), (E, cost=3), (D, cost=6)]`\n",
    "4. `[(E, cost=3), (D, cost=6), (E, cost=5), (F, cost=8)]`,    5. `[(D, cost=6), (E, cost=5), (F, cost=8), (G*, cost=4)]`\n",
    "\n",
    "- **Depth-first search**\n",
    "- **Iterative deepening search**: \n",
    "```bash\n",
    "        A\n",
    "       / \\\n",
    "      B   C\n",
    "     / \\   \\\n",
    "    D   E   G\n",
    "\n",
    "```\n",
    "Depth = 0 :`A`, Depth = 1 :`A → B, A → C`, Depth = 2 :`A → B → D, A → B → E, A → C → G`\n",
    "- **Bidirectional search**: Can enormously reduce time complexity, but it is not always applicable and may require too much space.\n",
    "\n",
    "## Informed Search Methods\n",
    "\n",
    "Informed search methods may have access to a heuristic function $h(n)$ that estimates the cost of a solution from $n$.\n",
    "\n",
    "- **Best-first search**: A generic algorithm that selects a node for expansion according to an evaluation function.\n",
    "- **Greedy best-first search**: Expands nodes with minimal $h(n)$. It is not optimal but is often efficient.\n",
    "- **A\\* search**: Expands nodes with minimal $f(n) = g(n) + h(n)$. \n",
    "  - A\\* is complete and optimal, provided that $h(n)$ is admissible (for Tree-Search) or consistent (for Graph-Search). \n",
    "  - However, the space complexity of A\\* is still prohibitive.\n",
    "  - <img src=\"https://d18l82el6cdm1i.cloudfront.net/uploads/hevQ7EbwVU-output_prgol9.gif\" alt=\"Breadth-First Search\" width=\"250\">\n",
    "- **RBFS (Recursive Best-First Search)** and **SMA\\* (Simplified Memory-Bounded A\\*)**: Robust, optimal search algorithms that use limited amounts of memory. Given enough time, they can solve problems that A\\* cannot solve due to memory constraints.\n",
    "\n",
    "## Heuristic Search Performance\n",
    "\n",
    "The performance of heuristic search algorithms depends on the quality of the heuristic function. Good heuristics can be constructed by:\n",
    "- Relaxing the problem definition.\n",
    "- Storing precomputed solution costs for subproblems in a **pattern database**.\n",
    "- Learning from experience with the problem class.\n",
    "\n",
    "\n",
    "- **BFS and UCS** are exhaustive but memory-intensive.\n",
    "- **DFS and DLS** save memory but risk incompleteness.\n",
    "- **IDS** combines benefits of BFS and DFS for large state spaces.\n",
    "- **Bidirectional Search** is efficient but requires a well-defined goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Admissible vs. Consistent Heuristics**\n",
    "- **Admissible**: Never overestimates the cost to the goal.  \n",
    "- **Consistent**: Satisfies the triangle inequality ($ h(n) \\leq c(n, a, n') + h(n') $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "| Criterion        | Breadth-First      | Uniform-Cost          | Depth-First | Depth-Limited | Iterative Deepening | Bidirectional (if applicable) |\n",
    "|------------------|--------------------|------------------------|-------------|---------------|---------------------|-------------------------------|\n",
    "| **Complete?**    | Yes $^a$           | Yes $^{a,b}$           | No          | No            | Yes $^a$            | Yes $^{a,d}$                  |\n",
    "| **Time**         | $O(b^d)$          | $O(b^{1+C^*/\\epsilon})$ | $O(b^m)$    | $O(b^\\ell)$   | $O(b^d)$           | $O(b^{d/2})$                 |\n",
    "| **Space**        | $O(b^d)$          | $O(b^{1+C^*/\\epsilon})$ | $O(bm)$     | $O(b^\\ell)$   | $O(bd)$            | $O(b^{d/2})$                 |\n",
    "| **Optimal?**     | Yes $^c$           | Yes                   | No          | No            | Yes $^c$            | Yes $^{c,d}$                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Local Search Methods:**\n",
    "     - **Hill Climbing:** A greedy approach that moves towards increasing value but can get stuck at local maxima, ridges, or plateaus.\n",
    "     - **Simulated Annealing:** Allows downhill moves probabilistically to escape local maxima, with the probability decreasing as the \"temperature\" decreases over time.\n",
    "\n",
    "4. **Nondeterministic Environments:**\n",
    "   - Require **AND–OR search trees** to handle contingencies.\n",
    "\n",
    "5. **Partially Observable Environments:**\n",
    "   - Use **belief states** to represent the agent’s knowledge about the possible physical states it could be in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Methods:\n",
    "\n",
    "**Model Checking:** Enumerates all possible models to verify entailment. Simple and guarantees correctness but computationally expensive ($ O(2^n) $) for large KBs.\n",
    "\n",
    "**Logical Inference:** Uses rules like **Modus Ponens** and **Resolution** to derive new sentences.\n",
    "\n",
    "**Resolution:** A single, powerful inference rule for CNF sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **- Boolean/Propositional logic**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert Propositional Logic to CNF:\n",
    "1. **Eliminate Implications (⇒)**:\n",
    "   - `A ⇒ B` becomes `¬A ∨ B`.\n",
    "2. **Move NOTs (`¬`) Inside** (De Morgan's Laws):\n",
    "   - `¬(A ∨ B)` becomes `¬A ∧ ¬B`.\n",
    "   - `¬(A ∧ B)` becomes `¬A ∨ ¬B`.\n",
    "3. **Distribute OR (`∨`) Over AND (`∧`)**:\n",
    "   - `(A ∧ B) ∨ C` becomes `(A ∨ C) ∧ (B ∨ C)`.\n",
    "\n",
    "#### **Definite Clauses**\n",
    "- Form: `(A ∧ B ∧ …) ⇒ C` or `¬A ∨ ¬B ∨ … ∨ C`.\n",
    "- Advantage:\n",
    "  - Simplifies chaining proofs.\n",
    "  - Human-readable format.\n",
    "\n",
    "\n",
    "#### **Horn Clauses:**\n",
    "- A special subset of CNF with at most one positive literal.\n",
    "- Used in efficient inference methods (e.g., forward/backward chaining)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First order logic**:\n",
    "- Representation of **objects**, **relations**, and **facts** with **quantifiers** $∀x ∃y$ relation $(x, y)$.\n",
    "\n",
    "#### **FOL Reduction to Propositional Logic**\n",
    "1. **Universal Instantiation**:\n",
    "   - Drop (`∀`) and replace the variable with a specific object.\n",
    "\n",
    "2. **Existential Instantiation**:\n",
    "   - Replace (`∃`) with a **Skolem constant** (Just write C).\n",
    "\n",
    "#### **Resolution in First-Order Logic**\n",
    "1. **Convert FOL to CNF**\n",
    "\n",
    "2. **Negate the Query**\n",
    "\n",
    "3. **Apply Resolution**\n",
    "\n",
    "4. **Contradiction → Query Proven**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### **Example:**\n",
    "**Knowledge Base (KB):**\n",
    "- `∀y (∃x owns(y, x) ⇒ hasProperty(y))`\n",
    "- `owns(joost, house)`\n",
    "\n",
    "**Query**: Prove `hasProperty(joost)`.\n",
    "\n",
    "\n",
    "#### **Step-by-Step Solution:**\n",
    "1. **Convert KB to CNF**:\n",
    "   - `∀y (∃x owns(y, x) ⇒ hasProperty(y))`\n",
    "   - Eliminate `⇒`: `∀y (¬∃x owns(y, x) ∨ hasProperty(y))`.\n",
    "   - Move `¬` inside: `∀y ∀x (¬owns(y, x) ∨ hasProperty(y))`.\n",
    "   - Skolemize: `¬owns(y, f(y)) ∨ hasProperty(y)` (replace `∃x` with Skolem function `f(y)`).\n",
    "   - CNF: `¬owns(y, f(y)) ∨ hasProperty(y)`.\n",
    "\n",
    "2. **Add Query Negation**:\n",
    "   - Add `¬hasProperty(joost)` to KB.\n",
    "\n",
    "3. **Resolution**:\n",
    "   - Unify `y = joost` and resolve:\n",
    "     - From KB: `¬owns(joost, f(joost)) ∨ hasProperty(joost)`\n",
    "     - Negation: `¬hasProperty(joost)`\n",
    "   - Resolving gives `¬owns(joost, f(joost))`.\n",
    "   - Unify `owns(joost, house)` with `¬owns(joost, f(joost))` (substituting `f(joost) = house`).\n",
    "   - Contradiction: `owns(joost, house)` and `¬owns(joost, house)`.\n",
    "\n",
    "4. **Conclusion**:\n",
    "   - Contradiction proves the query: `hasProperty(joost)` is **true**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Mutex** in Graphplan: Forward Search with Heuristics $\\rightarrow$ identifies **conflicts** between actions or conditions\n",
    "- Examples of mutexes:  \n",
    "  - Two actions have contradictory effects.  \n",
    "  - Two actions need conditions that can’t both be true at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Agent Program Structure**\n",
    "1. **Percept Rules:**\n",
    "   - Triggered when the agent senses the world.\n",
    "   - Add new facts to the belief base.\n",
    "   - Example: `at(X) & passage(Y) → +link(X, Y)`\n",
    "\n",
    "2. **Program Rules:**\n",
    "   - Define reasoning processes and determine:\n",
    "     - Which actions are appropriate (intentions).\n",
    "     - Which goals to pursue (desires).\n",
    "     - What is true or false (beliefs).\n",
    "\n",
    "3. **Action Rules:**\n",
    "   - Define the effects of actions (postconditions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applications of Bayes' Rule**\n",
    "1. **Bayes' Rule**:\n",
    "   - $ P(A | B) = \\frac{P(B | A)P(A)}{P(B)} = \\frac{P(a \\land b)}{P(b)} $, where $ P(B) \\neq 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **Key Features of MDPs**:\n",
    "1. **Stationary Policies**:\n",
    "   - Optimal policies depend only on the current state, not the time step.\n",
    "2. **Utility of a State**:\n",
    "   - Sum of **discounted rewards** over time:\n",
    "     $$\n",
    "     \\text{Bellman Equation} U(s) = R(s) + \\gamma \\max_{a \\in A} \\sum_{s'} P(s'|s, a) U(s')\n",
    "     $$\n",
    "     $$\n",
    "     \\text{Value Iteration: Start with $U(s) = 0$ and iteratively update to convergence and reach optimal policy.}\n",
    "     $$\n",
    "\n",
    "\n",
    "- **MDPs**: Framework for sequential decisions in fully observable stochastic settings.\n",
    "- **POMDPs**: Extensions for partially observable environments.\n",
    "- **Key Algorithms**:\n",
    "  - **Value Iteration** and **Policy Iteration** for MDPs.\n",
    "  - **Belief State-based Approaches** for POMDPs.\n",
    "- Decision-making balances **risk, reward**, and **information gathering**.\n",
    "\n",
    "\n",
    "#### **Decision Strategies**:\n",
    "- **Finite Horizon**: Optimize decisions for a fixed number of steps into the future.\n",
    "- **Infinite Horizon**: Decisions are made without a fixed endpoint, focusing on long-term rewards.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Dynamic Programming (DP)**\n",
    "- **Goal**: Solve for optimal value function and policy.\n",
    "- **Approaches**:\n",
    "  - **Policy Iteration**:\n",
    "    1. Evaluate policy until convergence.\n",
    "    2. Improve policy.\n",
    "  - **Value Iteration**:\n",
    "    1. Partial policy evaluation (1 sweep).\n",
    "    2. Policy improvement.\n",
    "    3. Repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Differences Between Policy Iteration, Value Iteration, and Q-Value Iteration**\n",
    "\n",
    "\n",
    "| **Aspect**               | **Policy Iteration**                                          | **Value Iteration**                                              | **Q-Value Iteration**                                          |\n",
    "|--------------------------|-------------------------------------------------------------|------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Goal**                 | Find the **optimal policy** $ \\pi^* $.                    | Find the **optimal value function** $ V^*(s) $.                | Find the **optimal state-action value** $ Q^*(s, a) $.      |\n",
    "| **Representation**       | Explicitly maintains and updates a policy $ \\pi(a\\|s) $.   | Implicitly derives the policy from $ V(s) $.                  | Policy is derived implicitly from $ Q(s, a) $.              |\n",
    "| **Evaluation Step**      | Performs **full policy evaluation** (until convergence).    | Performs **partial policy evaluation** (1 iteration per cycle). | Updates $ Q(s,a) $ for each state-action pair in one step.  |\n",
    "| **Improvement Step**     | Improves the policy after full evaluation.                  | Combines policy evaluation and improvement in one equation.     | Directly updates $ Q(s,a) $ using Bellman optimality.       |\n",
    "| **Formula Used**         | Bellman equation for $ V(s) $.                            | Bellman optimality equation for $ V(s) $.                     | Bellman optimality equation for $ Q(s,a) $.                 |\n",
    "| **Algorithm Complexity** | Higher computational cost due to full evaluation per step.  | Faster convergence due to single-step evaluation.               | Similar to Value Iteration but operates in action space.      |\n",
    "| **Output**               | Optimal policy $ \\pi^*(a\\|s) $.                            | Optimal value function $ V^*(s) $.                            | Optimal state-action value function $ Q^*(s,a) $.           |\n",
    "| **Usage**                | Suitable for small state spaces.                            | Suitable for large state spaces with fewer actions.             | Preferred for environments where actions play a key role.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Policy Iteration** is slower per iteration but converges in fewer iterations.\n",
    "- **Value Iteration** is faster per iteration and simpler but requires more iterations to converge.\n",
    "- **Q-Value Iteration** is more computationally expensive due to operating on state-action pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **SFOL**\n",
    "\n",
    "#### **Operators in SFOL:**\n",
    "- **Belief Updates:**\n",
    "  - `+`: Add fact to the belief base.\n",
    "  - `-`: Remove fact from the belief base.\n",
    "\n",
    "- **Goals and Intentions:**\n",
    "  - `*`: Add goal (adopt).\n",
    "  - `~`: Remove goal (drop automatically when achieved).\n",
    "  - `_`: Add intention (action)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## **Prolog**\n",
    "\n",
    "**Rules**: Define relationships with conditions.\n",
    "   ```prolog\n",
    "   brother(X, Y) :- \n",
    "       male(X),\n",
    "       parent(P, X),\n",
    "       parent(P, Y),\n",
    "       X \\= Y.\n",
    "   ```\n",
    "\n",
    "#### **Substitution & Unification**:\n",
    "- **Unification**: Matches terms in a query with facts/rules. HERE PROLOG RETURN TRUE/FALSE\n",
    "- **Substitution**: Assigns variables specific values during unification. HERE PROLOG RETURN THE VALUE OF THE VARIABLE\n",
    "  - Example:\n",
    "    ```prolog\n",
    "    ?- parent(joost, X). %Query\n",
    "    X = sacha ; %Answer\n",
    "    X = leon. %Answer\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Working with Lists**:\n",
    "- Lists are structured as `[Head | Tail]`.\n",
    "- Example:\n",
    "  ```prolog\n",
    "  append([], X, X).\n",
    "  append([H|T], Y, [H|Z]) :- append(T, Y, Z).\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Assignment-specific Highlights**:\n",
    "\n",
    "1. **Family Relationships**:\n",
    "   - Use recursive rules for **ancestor(X, Y)**.\n",
    "   - Define **brother(X, Y)** and **sister(X, Y)** based on shared parents and gender.\n",
    "\n",
    "2. **Cousins**:\n",
    "   ```prolog\n",
    "   cousin(X, Y) :-\n",
    "       parent(P1, X),\n",
    "       parent(P2, Y),\n",
    "       (brother(P1, P2); sister(P1, P2)),\n",
    "       X \\= Y.\n",
    "   ```\n",
    "\n",
    "3. **Family Predicate**:\n",
    "   ```prolog\n",
    "   family(X, Y) :- ancestor(X, Y); ancestor(Y, X); (ancestor(Z, X), ancestor(Z, Y)).\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Simple Reflexive Agent**:\n",
    "1. **Environment Representation**:\n",
    "   - Define locations as `link(X, Y)` facts.\n",
    "   - Represent the robot and goal:\n",
    "     ```prolog\n",
    "     robot(1).\n",
    "     goal(5).\n",
    "     ```\n",
    "\n",
    "2. **Basic Agent Behavior**:\n",
    "   - **Adjacent**:\n",
    "     ```prolog\n",
    "     adjacent(Y) :- link(X, Y), robot(X).\n",
    "     ```\n",
    "   - **Move**:\n",
    "     ```prolog\n",
    "     move(Y) :- adjacent(Y), retract(robot(_)), assert(robot(Y)).\n",
    "     ```\n",
    "\n",
    "3. **Pathfinding**:\n",
    "   - Use recursion to find paths:\n",
    "     ```prolog\n",
    "     path(G, [G]) :- goal(G).\n",
    "     path(X, [X|Rest]) :- link(X, Z), path(Z, Rest).\n",
    "     ```\n",
    "\n",
    "4. **Improved Suggestions**:\n",
    "   - Suggest next step in path:\n",
    "     ```prolog\n",
    "     suggest(Next) :-\n",
    "         robot(X),\n",
    "         path([X, Next | _]).\n",
    "     ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
