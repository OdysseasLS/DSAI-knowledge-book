{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 17: **Making Complex Decisions**\n",
    "\n",
    "### **17.1 Sequential Decision Problems**\n",
    "- **Focus**: Computational decision-making in **stochastic environments**.\n",
    "- **Sequential Problems**:\n",
    "  - Decisions depend on sequences of actions, unlike episodic problems where outcomes are independent.\n",
    "  - Incorporates **uncertainty, sensing**, and **utilities**.\n",
    "  - Examples include Markov Decision Processes (MDPs).\n",
    "  \n",
    "#### **Markov Decision Processes (MDPs)**:\n",
    "- Defined for **fully observable, stochastic environments**:\n",
    "  1. **States (S)**: The environment's possible configurations.\n",
    "  2. **Actions (A)**: Choices available in each state.\n",
    "  3. **Transition Model (P(s'|s, a))**: Probability of reaching state `s'` from `s` after action `a`.\n",
    "  4. **Reward Function (R(s))**: Reward for being in state `s`.\n",
    "\n",
    "- **Policy (π)**:\n",
    "  - A strategy specifying the best action `π(s)` for each state.\n",
    "  - **Optimal Policy (π*)** maximizes the **expected utility** of the sequence.\n",
    "\n",
    "#### **Key Features of MDPs**:\n",
    "1. **Stationary Policies**:\n",
    "   - Optimal policies depend only on the current state, not the time step.\n",
    "2. **Utility of a State**:\n",
    "   - Sum of **discounted rewards** over time:\n",
    "     $$\n",
    "     U(s) = R(s) + \\gamma \\max_{a \\in A} \\sum_{s'} P(s'|s, a) U(s')\n",
    "     $$\n",
    "   - `γ`: Discount factor (0 ≤ γ ≤ 1) prioritizing immediate rewards over future ones.\n",
    "\n",
    "#### **Decision Strategies**:\n",
    "- **Finite Horizon**: Decisions optimized over a fixed number of steps.\n",
    "- **Infinite Horizon**: No deadline; stationary policies apply.\n",
    "\n",
    "---\n",
    "\n",
    "### **17.2 Value Iteration**\n",
    "- **Goal**: Compute state utilities and derive optimal policies.\n",
    "\n",
    "#### **Bellman Equation**:\n",
    "- Core formula linking a state's utility to its neighbors:\n",
    "  $$\n",
    "  U(s) = R(s) + \\gamma \\max_{a \\in A(s)} \\sum_{s'} P(s'|s, a) U(s')\n",
    "  $$\n",
    "\n",
    "#### **Value Iteration Algorithm**:\n",
    "1. Initialize utilities arbitrarily (e.g., U(s) = 0).\n",
    "2. Update iteratively using the Bellman equation.\n",
    "3. Stop when changes in utilities are smaller than a given threshold.\n",
    "\n",
    "- **Convergence**:\n",
    "  - Guaranteed due to the **contraction property** of Bellman updates.\n",
    "  - Error bounds ensure policies are nearly optimal after finite iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### **17.3 Policy Iteration**\n",
    "- **Goal**: Directly improve policies without exact utilities.\n",
    "1. **Policy Evaluation**:\n",
    "   - Compute utilities under the current policy.\n",
    "   - Linear equations are solved for all states.\n",
    "2. **Policy Improvement**:\n",
    "   - Update the policy using one-step look-ahead based on current utilities.\n",
    "3. Repeat until the policy stabilizes (converges).\n",
    "\n",
    "- **Modified Policy Iteration**:\n",
    "  - Combines value iteration and policy evaluation for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **MDPs**: Framework for sequential decisions in fully observable stochastic settings.\n",
    "- **POMDPs**: Extensions for partially observable environments.\n",
    "- **Key Algorithms**:\n",
    "  - **Value Iteration** and **Policy Iteration** for MDPs.\n",
    "  - **Belief State-based Approaches** for POMDPs.\n",
    "- Decision-making balances **risk, reward**, and **information gathering**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
