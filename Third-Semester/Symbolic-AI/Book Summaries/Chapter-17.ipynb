{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 17: **Making Complex Decisions**\n",
    "\n",
    "- Sequential decision-making in **stochastic environments** $\\rightarrow$ Incorporates **uncertainty, sensing**, and **utilities** $\\rightarrow$ MDP\n",
    "  \n",
    "#### **Markov Decision Processes (MDPs)**:\n",
    "\n",
    "**Framework for sequential decisions in fully observable stochastic settings.**\n",
    "\n",
    "- **Policy (π)**:\n",
    "  - A strategy specifying the best action $`π(s)`$ for each state.\n",
    "  - **Optimal Policy $(π*)$** maximizes the **expected utility** of the sequence.\n",
    "\n",
    "#### **Key Features of MDPs**:\n",
    "1. **Stationary Policies**:\n",
    "   - Optimal policies depend only on the current state, not the time step.\n",
    "2. **Utility of a State**:\n",
    "   - Sum of **discounted rewards** over time:\n",
    "     $$\n",
    "     \\text{Bellman Equation} U(s) = R(s) + \\gamma \\max_{a \\in A} \\sum_{s'} P(s'|s, a) U(s')\n",
    "     $$\n",
    "     $$\n",
    "     \\text{Value Iteration: Start with $U(s) = 0$ and iteratively update to convergence and reach optimal policy.}\n",
    "     $$\n",
    "\n",
    "#### **Decision Strategies**:\n",
    "- **Finite Horizon**: Optimize decisions for a fixed number of steps into the future.\n",
    "- **Infinite Horizon**: Decisions are made without a fixed endpoint, focusing on long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "- **MDPs**: Framework for sequential decisions in fully observable stochastic settings.\n",
    "- **POMDPs**: Extensions for partially observable environments.\n",
    "- **Key Algorithms**:\n",
    "  - **Value Iteration** and **Policy Iteration** for MDPs.\n",
    "  - **Belief State-based Approaches** for POMDPs.\n",
    "- Decision-making balances **risk, reward**, and **information gathering**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
