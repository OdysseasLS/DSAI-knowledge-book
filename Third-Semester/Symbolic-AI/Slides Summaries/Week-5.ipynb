{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dynamic Programming (DP)**\n",
    "- **Goal**: Solve for optimal value function and policy.\n",
    "- **Approaches**:\n",
    "  - **Policy Iteration**:\n",
    "    1. Evaluate policy until convergence.\n",
    "    2. Improve policy.\n",
    "  - **Value Iteration**:\n",
    "    1. Partial policy evaluation (1 sweep).\n",
    "    2. Policy improvement.\n",
    "    3. Repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy tells the robot which action to take in each state.\n",
    "For a deterministic policy, the robot always follows a fixed path (e.g., right â†’ down).\n",
    "For a stochastic policy, the robot may take different actions probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_{\\pi, T} \\left[ \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i} \\mid s_t = s \\right]\n",
    "$$\n",
    "\n",
    "### Explanation \n",
    "- **$ V^\\pi(s) $**: Expected cumulative reward starting from state $ s $ while following policy $ \\pi $.\n",
    "- **$ \\mathbb{E}_{\\pi, T} $**: Expectation over trajectories based on policy $ \\pi $ and transition $ T $.\n",
    "- **$ \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i} $**: Sum of discounted rewards:\n",
    "  - $ \\gamma $: Discount factor ($ 0 \\leq \\gamma \\leq 1 $).\n",
    "  - $ r_{t+i} $: Reward at time step $ t+i $.\n",
    "- The calculation starts at $ s_t = s $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### State and State-Action Values in MDPs\n",
    "\n",
    "1. **State Value ($ V(s) $)**:\n",
    "   - **Formula**:\n",
    "     $$\n",
    "     V^\\pi(s) = \\mathbb{E}_{\\pi, T} \\left[ \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i} \\mid s_t = s \\right]\n",
    "     $$\n",
    "\n",
    "2. **State-Action Value ($ Q(s,a) $)**:\n",
    "   - **Formula**:\n",
    "     $$\n",
    "     Q^\\pi(s, a) = \\mathbb{E}_{\\pi, T} \\left[ r(s,a) + \\gamma \\cdot V^\\pi(s') \\right]\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Bellman Equation**\n",
    "\n",
    "### **State Value Function $ V(s) $**\n",
    "- **Formula**:\n",
    "  $$\n",
    "  V^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} T(s'|s,a) \\left[ R(s,a,s') + \\gamma V^\\pi(s') \\right]\n",
    "  $$\n",
    "  \n",
    "\n",
    "\n",
    "### **State-Action Value Function $ Q(s, a) $**\n",
    "- **Formula**:\n",
    "  $$\n",
    "  Q^\\pi(s, a) = \\sum_{s' \\in S} T(s'|s,a) \\left[ R(s,a,s') + \\gamma \\sum_{a' \\in A} \\pi(a'|s') Q^\\pi(s',a') \\right]\n",
    "  $$\n",
    "  \n",
    "### **Backup Diagrams**\n",
    "\n",
    "![Graph](../../../Files/third-semester/sai/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Dynamic Programming\n",
    "#### Key idea:\n",
    "- Break a large problem into smaller subproblems.\n",
    "- Efficiently store and reuse intermediate results.\n",
    "- Repeatedly solving the small subproblem solves the overall problem.\n",
    "\n",
    "In context of MDP: a central algorithm to solve for the **optimal policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "#### **Curse of Dimensionality**:\n",
    "   - The number of states grows exponentially with the number of variables.\n",
    "   - Example: Tic-tac-toe (3x3 board):\n",
    "     - $ 3^9 = 19,683 $ unique states.\n",
    "   - 4x4 board:\n",
    "     - $ 3^{16} = 43,046,721 $ states.\n",
    "   - High memory and computational requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### **Differences Between Policy Iteration, Value Iteration, and Q-Value Iteration**\n",
    "\n",
    "\n",
    "| **Aspect**               | **Policy Iteration**                                          | **Value Iteration**                                              | **Q-Value Iteration**                                          |\n",
    "|--------------------------|-------------------------------------------------------------|------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Goal**                 | Find the **optimal policy** $ \\pi^* $.                    | Find the **optimal value function** $ V^*(s) $.                | Find the **optimal state-action value** $ Q^*(s, a) $.      |\n",
    "| **Representation**       | Explicitly maintains and updates a policy $ \\pi(a\\|s) $.   | Implicitly derives the policy from $ V(s) $.                  | Policy is derived implicitly from $ Q(s, a) $.              |\n",
    "| **Evaluation Step**      | Performs **full policy evaluation** (until convergence).    | Performs **partial policy evaluation** (1 iteration per cycle). | Updates $ Q(s,a) $ for each state-action pair in one step.  |\n",
    "| **Improvement Step**     | Improves the policy after full evaluation.                  | Combines policy evaluation and improvement in one equation.     | Directly updates $ Q(s,a) $ using Bellman optimality.       |\n",
    "| **Formula Used**         | Bellman equation for $ V(s) $.                            | Bellman optimality equation for $ V(s) $.                     | Bellman optimality equation for $ Q(s,a) $.                 |\n",
    "| **Algorithm Complexity** | Higher computational cost due to full evaluation per step.  | Faster convergence due to single-step evaluation.               | Similar to Value Iteration but operates in action space.      |\n",
    "| **Output**               | Optimal policy $ \\pi^*(a\\|s) $.                            | Optimal value function $ V^*(s) $.                            | Optimal state-action value function $ Q^*(s,a) $.           |\n",
    "| **Usage**                | Suitable for small state spaces.                            | Suitable for large state spaces with fewer actions.             | Preferred for environments where actions play a key role.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- **Policy Iteration** is slower per iteration but converges in fewer iterations.\n",
    "- **Value Iteration** is faster per iteration and simpler but requires more iterations to converge.\n",
    "- **Q-Value Iteration** is more computationally expensive due to operating on state-action pairs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
