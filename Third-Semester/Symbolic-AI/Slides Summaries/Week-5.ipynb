{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sequential Decision Making**\n",
    "- Many AI problems are sequential (e.g., shortest-path problems).\n",
    "\n",
    "\n",
    "\n",
    "### **Markov Decision Process (MDP)**\n",
    "| **Component**              | **Symbol**   | **Description**                     |\n",
    "|----------------------------|--------------|-------------------------------------|\n",
    "| State Space                | S            | Possible observations.              |\n",
    "| Action Space               | A            | Possible actions.                   |\n",
    "| Transition Function        | T(s’|s,a)    | Environment's response.             |\n",
    "| Reward Function            | R(s,a,s’)    | Transition's desirability.          |\n",
    "| Discount Factor            | $\\gamma$     | Weighting future rewards.           |\n",
    "| Initial State Distribution | p₀(s)        | Starting probabilities.             |\n",
    "| Policy                     | $π(a\\|s)$     | Strategy for selecting actions.     |\n",
    "\n",
    "### **Dynamic Programming (DP)**\n",
    "- **Goal**: Solve for optimal value function and policy.\n",
    "- **Approaches**:\n",
    "  - **Policy Iteration**:\n",
    "    1. Evaluate policy until convergence.\n",
    "    2. Improve policy.\n",
    "  - **Value Iteration**:\n",
    "    1. Partial policy evaluation (1 sweep).\n",
    "    2. Policy improvement.\n",
    "    3. Repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy tells the robot which action to take in each state.\n",
    "For a deterministic policy, the robot always follows a fixed path (e.g., right → down).\n",
    "For a stochastic policy, the robot may take different actions probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_{\\pi, T} \\left[ \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i} \\mid s_t = s \\right]\n",
    "$$\n",
    "\n",
    "### Explanation \n",
    "- **$ V^\\pi(s) $**: Expected cumulative reward starting from state $ s $ while following policy $ \\pi $.\n",
    "- **$ \\mathbb{E}_{\\pi, T} $**: Expectation over trajectories based on policy $ \\pi $ and transition $ T $.\n",
    "- **$ \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i} $**: Sum of discounted rewards:\n",
    "  - $ \\gamma $: Discount factor ($ 0 \\leq \\gamma \\leq 1 $).\n",
    "  - $ r_{t+i} $: Reward at time step $ t+i $.\n",
    "- The calculation starts at $ s_t = s $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### State and State-Action Values in MDPs\n",
    "\n",
    "1. **State Value ($ V(s) $)**:\n",
    "   - **Definition**: The expected cumulative reward starting from state $ s $, following policy $ \\pi $.\n",
    "   - **Formula**:\n",
    "     $$\n",
    "     V^\\pi(s) = \\mathbb{E}_{\\pi, T} \\left[ \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i} \\mid s_t = s \\right]\n",
    "     $$\n",
    "   - **Representation**:\n",
    "     - Stored as a **vector** of size $|S|$, where each entry corresponds to a state.\n",
    "     - Example:\n",
    "       $$\n",
    "       V(s) = \n",
    "       \\begin{bmatrix}\n",
    "       V(1) = 3, \\\\\n",
    "       V(2) = -4, \\\\\n",
    "       V(3) = 9, \\dots\n",
    "       \\end{bmatrix}\n",
    "       $$\n",
    "\n",
    "2. **State-Action Value ($ Q(s,a) $)**:\n",
    "   - **Definition**: The expected cumulative reward starting from state $ s $, taking action $ a $, and then following policy $ \\pi $.\n",
    "   - **Formula**:\n",
    "     $$\n",
    "     Q^\\pi(s, a) = \\mathbb{E}_{\\pi, T} \\left[ r(s,a) + \\gamma \\cdot V^\\pi(s') \\right]\n",
    "     $$\n",
    "   - **Representation**:\n",
    "     - Stored as a **matrix** of size $|S| \\times |A|$, where each entry corresponds to a state-action pair.\n",
    "     - Example:\n",
    "       $$\n",
    "       Q(s,a) = \n",
    "       \\begin{bmatrix}\n",
    "       Q(1, \\text{up}) = 5 & Q(1, \\text{down}) = 3 & \\dots \\\\\n",
    "       Q(2, \\text{up}) = 9 & Q(2, \\text{down}) = 4 & \\dots\n",
    "       \\end{bmatrix}\n",
    "       $$\n",
    "\n",
    "### **Optimal Value and Policy:**\n",
    "\n",
    "1. **Optimal Value Function ($ V^*(s) $)**:\n",
    "   - The maximum value achievable in state $ s $ under any policy.\n",
    "   - $ V^*(s) = \\max_{\\pi} V^\\pi(s) $.\n",
    "\n",
    "2. **Optimal Policy ($ \\pi^* $)**:\n",
    "   - The policy that achieves $ V^*(s) $.\n",
    "   - $ \\pi^*(s) = \\arg\\max_a Q^*(s, a) $, where $ Q^*(s, a) $ is the optimal state-action value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Bellman Equation\n",
    "\n",
    "\n",
    "\n",
    "### **Bellman Equation**\n",
    "- A **recursive formula** for the value function $ V(s) $ or state-action value $ Q(s, a) $.\n",
    "- It relates the value of a state to the values of its possible next states.\n",
    "\n",
    "\n",
    "\n",
    "### **State Value Function $ V(s) $**\n",
    "- **Formula**:\n",
    "  $$\n",
    "  V^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S} T(s'|s,a) \\left[ R(s,a,s') + \\gamma V^\\pi(s') \\right]\n",
    "  $$\n",
    "- **Explanation**:\n",
    "  - $ \\pi(a|s) $: Probability of taking action $ a $ in state $ s $.\n",
    "  - $ T(s'|s,a) $: Probability of transitioning to $ s' $ from $ s $ by taking $ a $.\n",
    "  - $ R(s,a,s') $: Reward for transitioning from $ s $ to $ s' $ via $ a $.\n",
    "  - $ \\gamma $: Discount factor.\n",
    "\n",
    "\n",
    "\n",
    "### **State-Action Value Function $ Q(s, a) $**\n",
    "- **Formula**:\n",
    "  $$\n",
    "  Q^\\pi(s, a) = \\sum_{s' \\in S} T(s'|s,a) \\left[ R(s,a,s') + \\gamma \\sum_{a' \\in A} \\pi(a'|s') Q^\\pi(s',a') \\right]\n",
    "  $$\n",
    "- **Explanation**:\n",
    "  - Combines the immediate reward $ R(s,a,s') $ and the discounted future value $ Q(s',a') $.\n",
    "\n",
    "\n",
    "### **Backup Diagrams**\n",
    "\n",
    "![Graph](../../Files/third-semester/sai/3.png)\n",
    "\n",
    "- $ V(s) $ and $ Q(s, a) $ are interconvertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Dynamic Programming\n",
    "#### Key idea:\n",
    "- Break a large problem into smaller subproblems.\n",
    "- Efficiently store and reuse intermediate results.\n",
    "- Repeatedly solving the small subproblem solves the overall problem.\n",
    "\n",
    "In context of MDP: a central algorithm to solve for the **optimal policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![DP](../../Files/third-semester/sai/4.png)\n",
    "![DP](../../Files/third-semester/sai/5.png)\n",
    "\n",
    "### **Policy Evaluation**\n",
    "![ DP](../../Files/third-semester/sai/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The **optimal policy** is greedy because it selects the action that **optimizes the immediate and future rewards** (based on the Bellman equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Dynamic Programming Notes**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Two Approaches**\n",
    "1. **Policy Iteration**:\n",
    "   - Step 1: Policy Evaluation (until convergence).\n",
    "   - Step 2: Policy Improvement.\n",
    "\n",
    "2. **Value Iteration**:\n",
    "   - Step 1: Policy Evaluation (1 cycle per iteration).\n",
    "   - Step 2: Policy Improvement.\n",
    "   - Both steps alternate until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Value Iteration Algorithm**\n",
    "- Combines **policy evaluation** and **improvement** in one equation.\n",
    "- **Loop until convergence**:\n",
    "  1. For each state $ s $:\n",
    "     $$\n",
    "     V(s) = \\max_a \\sum_{s'} T(s'|s,a) \\left[ R(s,a,s') + \\gamma V(s') \\right]\n",
    "     $$\n",
    "- Returns the **optimal value function** $ V^*(s) $.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Q-Value Iteration**\n",
    "- Similar to value iteration but uses **state-action values**.\n",
    "- **Algorithm**:\n",
    "  1. Loop until convergence:\n",
    "     $$\n",
    "     Q(s,a) = \\sum_{s'} T(s'|s,a) \\left[ R(s,a,s') + \\gamma \\max_{a'} Q(s',a') \\right]\n",
    "     $$\n",
    "  2. Returns $ Q^*(s,a) $: optimal state-action values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Implicit Policies**\n",
    "- Policies derived from value tables instead of being explicitly stored.\n",
    "- **Example**:\n",
    "  - Value Table:\n",
    "    $$\n",
    "    s: \\{1, 2, 3\\}, \\quad V(s): \\{12, 24, 33\\}\n",
    "    $$\n",
    "  - Greedy policy selects actions maximizing $ V(s) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: Policy Iteration vs Value Iteration**\n",
    "| **Aspect**              | **Policy Iteration**               | **Value Iteration**                |\n",
    "|-------------------------|------------------------------------|------------------------------------|\n",
    "| **Policy Evaluation**   | Full convergence per iteration.   | One cycle per iteration.          |\n",
    "| **Efficiency**          | Slower due to full evaluation.    | Faster but requires more iterations. |\n",
    "| **Complexity**          | Higher computational cost.        | Lower computational cost per step.|\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Challenges**\n",
    "1. **Curse of Dimensionality**:\n",
    "   - The number of states grows exponentially with the number of variables.\n",
    "   - Example: Tic-tac-toe (3x3 board):\n",
    "     - $ 3^9 = 19,683 $ unique states.\n",
    "   - 4x4 board:\n",
    "     - $ 3^{16} = 43,046,721 $ states.\n",
    "   - High memory and computational requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "1. **Markov Decision Process (MDP)**:\n",
    "   - Framework for sequential tasks.\n",
    "   - Handles stochastic dynamics and multiple goals.\n",
    "2. **Bellman Equation**:\n",
    "   - Recursive relation between state/state-action values.\n",
    "   - Basis for MDP algorithms.\n",
    "3. **Dynamic Programming**:\n",
    "   - Policy Iteration: Alternates between evaluation and improvement.\n",
    "   - Value Iteration: Combines evaluation and improvement in one step.\n",
    "4. **Applications**:\n",
    "   - Key principles in AI for search, planning, and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "### **Differences Between Policy Iteration, Value Iteration, and Q-Value Iteration**\n",
    "\n",
    "\n",
    "| **Aspect**               | **Policy Iteration**                                          | **Value Iteration**                                              | **Q-Value Iteration**                                          |\n",
    "|--------------------------|-------------------------------------------------------------|------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Goal**                 | Find the **optimal policy** $ \\pi^* $.                    | Find the **optimal value function** $ V^*(s) $.                | Find the **optimal state-action value** $ Q^*(s, a) $.      |\n",
    "| **Representation**       | Explicitly maintains and updates a policy $ \\pi(a\\|s) $.   | Implicitly derives the policy from $ V(s) $.                  | Policy is derived implicitly from $ Q(s, a) $.              |\n",
    "| **Evaluation Step**      | Performs **full policy evaluation** (until convergence).    | Performs **partial policy evaluation** (1 iteration per cycle). | Updates $ Q(s,a) $ for each state-action pair in one step.  |\n",
    "| **Improvement Step**     | Improves the policy after full evaluation.                  | Combines policy evaluation and improvement in one equation.     | Directly updates $ Q(s,a) $ using Bellman optimality.       |\n",
    "| **Formula Used**         | Bellman equation for $ V(s) $.                            | Bellman optimality equation for $ V(s) $.                     | Bellman optimality equation for $ Q(s,a) $.                 |\n",
    "| **Algorithm Complexity** | Higher computational cost due to full evaluation per step.  | Faster convergence due to single-step evaluation.               | Similar to Value Iteration but operates in action space.      |\n",
    "| **Output**               | Optimal policy $ \\pi^*(a\\|s) $.                            | Optimal value function $ V^*(s) $.                            | Optimal state-action value function $ Q^*(s,a) $.           |\n",
    "| **Usage**                | Suitable for small state spaces.                            | Suitable for large state spaces with fewer actions.             | Preferred for environments where actions play a key role.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### **Key Points from Lecture Notes: Discrete Markov Decision Processes and Utility**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Preliminaries**\n",
    "- **Sets**: Discrete (e.g., {up, down}) or continuous (e.g., $[0, 1]$).\n",
    "- **Functions**: Map from domain $X$ to co-domain $Y$ ($f: X \\to Y$).\n",
    "- **Probability**: \n",
    "  - Probability distribution $p(X)$ sums to 1.\n",
    "  - Conditional probability: $p(X|Y)$.\n",
    "- **Expectation**: Weighted average of outcomes:\n",
    "  $$\n",
    "  E[f(X)] = \\sum_x p(x) \\cdot f(x)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Markov Decision Processes (MDPs)**\n",
    "- **Components**:\n",
    "  1. **State space** ($S$): Set of possible observations.\n",
    "  2. **Action space** ($A$): Set of possible actions.\n",
    "  3. **Transition function** ($T(s'|s, a)$): Probability of reaching $s'$ from $s$ using $a$.\n",
    "  4. **Reward function** ($R(s, a, s')$): Reward for transitioning $s \\to s'$.\n",
    "  5. **Discount factor** ($\\gamma$): Weight for future rewards ($[0, 1]$).\n",
    "  6. **Initial state distribution** ($p_0(s)$): Starting probabilities.\n",
    "- **Goals**:\n",
    "  - Maximize cumulative rewards (utility).\n",
    "  - Handle stochastic environments and multiple objectives.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Policies**\n",
    "- **Policy** ($\\pi(a|s)$): Specifies action probabilities in each state.\n",
    "- **Deterministic Policy**: Always selects a single action for each state ($\\pi(s) = a$).\n",
    "- **Greedy Policy**: Selects the action with the highest value.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Cumulative Reward and Value**\n",
    "- **Cumulative Reward (Return)**:\n",
    "  $$\n",
    "  G_t = \\sum_{i=0}^\\infty \\gamma^i \\cdot r_{t+i}\n",
    "  $$\n",
    "- **Value Function**:\n",
    "  - **State Value ($V^\\pi(s)$)**: Expected return starting from $s$ under policy $\\pi$:\n",
    "    $$\n",
    "    V^\\pi(s) = E[G_t | s_t = s, \\pi]\n",
    "    $$\n",
    "  - **State-Action Value ($Q^\\pi(s, a)$)**: Expected return starting from $s$, taking $a$, and then following $\\pi$:\n",
    "    $$\n",
    "    Q^\\pi(s, a) = E[G_t | s_t = s, a_t = a, \\pi]\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Optimal Policy and Value Function**\n",
    "- **Optimal Value Function ($V^*(s)$)**: Maximum possible value:\n",
    "  $$\n",
    "  V^*(s) = \\max_\\pi V^\\pi(s)\n",
    "  $$\n",
    "- **Optimal Policy ($\\pi^*$)**: Always greedy with respect to $V^*(s)$ or $Q^*(s, a)$:\n",
    "  $$\n",
    "  \\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Bellman Equations**\n",
    "- **State Value ($V(s)$)**:\n",
    "  $$\n",
    "  V(s) = \\max_a \\sum_{s'} T(s'|s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "  $$\n",
    "- **State-Action Value ($Q(s, a)$)**:\n",
    "  $$\n",
    "  Q(s, a) = \\sum_{s'} T(s'|s, a) \\left[ R(s, a, s') + \\gamma \\max_{a'} Q(s', a') \\right]\n",
    "  $$\n",
    "- Relation:\n",
    "  $$\n",
    "  V(s) = \\max_a Q(s, a)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Dynamic Programming (DP)**\n",
    "- **Core Idea**: Iteratively improve policy and value functions.\n",
    "- **Two Main Approaches**:\n",
    "  1. **Policy Iteration**:\n",
    "     - Alternate between:\n",
    "       - Policy evaluation: Compute $V^\\pi(s)$ for a given $\\pi$.\n",
    "       - Policy improvement: Update $\\pi(s) = \\arg\\max_a Q(s, a)$.\n",
    "     - Repeat until convergence.\n",
    "  2. **Value Iteration**:\n",
    "     - Update $V(s)$ directly using Bellman optimality equation:\n",
    "       $$\n",
    "       V(s) = \\max_a \\sum_{s'} T(s'|s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n",
    "       $$\n",
    "     - Derive policy from $V(s)$ after convergence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Algorithms**\n",
    "1. **Policy Iteration**:\n",
    "   - Evaluate $V(s)$ for $\\pi$ until convergence.\n",
    "   - Improve $\\pi(s)$ based on $Q(s, a)$.\n",
    "2. **Value Iteration**:\n",
    "   - Directly update $V(s)$ for each state until convergence.\n",
    "   - Derive policy after $V(s)$ converges.\n",
    "3. **Q-Value Iteration**:\n",
    "   - Directly update $Q(s, a)$ for state-action pairs:\n",
    "     $$\n",
    "     Q(s, a) = \\sum_{s'} T(s'|s, a) \\left[ R(s, a, s') + \\gamma \\max_{a'} Q(s', a') \\right]\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Practical Considerations**\n",
    "- **Dynamic Programming Strengths**:\n",
    "  - Guaranteed to find the optimal solution.\n",
    "  - No heuristic required.\n",
    "- **Weaknesses**:\n",
    "  - High memory requirements ($O(|S|)$ or $O(|S| \\times |A|)$).\n",
    "  - Inefficient for large state spaces.\n",
    "\n",
    "---\n",
    "\n",
    "#### **10. Key Insights for Exam**\n",
    "- Understand MDP components (state space, action space, transition, reward).\n",
    "- Derive and use Bellman equations for $V(s)$ and $Q(s, a)$.\n",
    "- Differentiate between Policy Iteration, Value Iteration, and Q-Value Iteration.\n",
    "- Understand dynamic programming workflow and greedy policies.\n",
    "- Compute values using cumulative rewards and expectations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
