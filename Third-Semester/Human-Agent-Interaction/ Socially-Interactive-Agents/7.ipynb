{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 7: Gesture Generation**\n",
    "\n",
    "### **Classification of Gestures**\n",
    "   - **Emblems**: Standalone gestures (e.g., thumbs up) that replace speech. Their meaning is culture-dependent.\n",
    "   - **Beats**: Simple gestures that emphasize speech rhythm without conveying specific meaning.\n",
    "   - **Iconic gestures**: Represent physical objects or actions (e.g., mimicking cutting with a knife).\n",
    "   - **Deictic gestures**: Pointing gestures that direct attention to something specific.\n",
    "   - **Metaphoric gestures**: Represent abstract concepts (e.g., cupping hands to represent \"all ideas\").\n",
    "\n",
    "### **Gestural Phases**:\n",
    " Rest, preparation, stroke (main part of the gesture), hold, and relax phases. These phases may flow into each other in a gesture sequence.\n",
    "\n",
    "### 7. **Rule-based vs. Data-driven Approaches**\n",
    "   - **Rule-based models**: Use predefined rules and linguistic information to generate gestures.\n",
    "      - **Cerebella Architecture**:\n",
    "      - A hybrid system that uses both **acoustic** and **linguistic elements** to dynamically generate gestures with both the speech's auditory cues and its semantic structure.\n",
    "      - Used in both **virtual agent** and **social robotics**.\n",
    "      - Divides its process into stages:\n",
    "      1. Input treatment (text and audio processing).\n",
    "      2. Communicative functions (CFs) like emotion, emphasis analysis.\n",
    "      3. Behavior mapping based on the context.\n",
    "      4. Animation scheduling.\n",
    "\n",
    "   - **GRETA Architecture**:\n",
    "      - Uses **high-level concepts** and external context (e.g., **dialogue goals** or **user moves**) to influence gesture generation by **communicative intentions**\n",
    "      \n",
    "      **Workflow**:\n",
    "      1. **User Input**: The user speaks or interacts with the agent.\n",
    "      2. **MIND**: The system processes this input, deciding the agent’s emotional tone and what it should say.\n",
    "      3. **MIDAS**: Adds details to the agent’s response by selecting appropriate gestures and expressions that match the speech.\n",
    "      4. **BODY**: The chosen gestures are generated and timed perfectly with the speech for a natural response.\n",
    "      5. **User Model & Context Features**: Throughout, the system considers the user's preferences, reactions, and the external context to tailor the response, ensuring it fits the conversation setting.\n",
    "   - **Data-driven models**: Use machine learning to learn gestures from large datasets of audio and video, producing more natural gestures but with less nuanced communication."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
