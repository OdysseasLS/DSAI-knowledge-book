{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chapter 7: Verbal Interaction**\n",
    "\n",
    "### **7.1 Human-Human Verbal Interaction**\n",
    "\n",
    "- **Speech Functions**:\n",
    "  - Conveying information\n",
    "  - Creating shared reality and joint attention: It allows people to synchronize their thoughts or actions, making communication more meaningful and collaborative.\n",
    "  - Complex and context-dependent (intonation(The way you say something), emphasis, and prosody((the rhythm and tone of how something is said)) affect meaning)\n",
    "  \n",
    "- **Components of Speech**:\n",
    "  - **Utterance**: smallest unit of spoken language like full sentence, a phrase, or even just a word, informal and may contain conversational fillers or incomplete grammar.\n",
    "  - **Words**: Smallest units conveying meaning.\n",
    "  - **Phonemes**: Smallest sound units that make up words. Example: \"pat\" = [p] [a] [t].\n",
    "  - **Conversational Fillers**: Uh-huh, uhm, used to maintain conversation flow.\n",
    "  - **Paralinguistic Information**: Non-verbal signals (intonation, gestures, gaze) that add meaning to verbal communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7.2 Speech Recognition**\n",
    "\n",
    "- **Automatic Speech Recognition (ASR)**: Converts spoken language into written text (speech-to-text).\n",
    "  - **Challenges in HRI**: Noise, diverse accents, distance from the microphone, and noisy environments.\n",
    "  - **Speech Recognition Process**:\n",
    "    - Speech recorded in the **time domain** and converted to the **frequency domain** for analysis.\n",
    "      - While the time domain shows how loud the speech is at any given time, the frequency domain breaks down the speech into its individual frequencies, which is much more useful for understanding the specific sounds (phonemes) being produced.\n",
    "    - Deep learning and **sequence-to-sequence models** (since 2010) have greatly improved ASR performance, replacing older models like Gaussian Mixture and Hidden Markov Models.\n",
    "    - Cloud-based ASR is more accurate but requires an internet connection.\n",
    "  \n",
    "- **McGurk Effect**: Shows how visual cues (like lip movements) influence what we hear. For example, if you hear \"ba\" but see someone saying \"fa,\" your brain might interpret the sound as \"fa.\", demonstrating **multimodal perception** in speech.\n",
    "\n",
    "- **Limitations of ASR**:\n",
    "  - Struggles with dialects, non-native speakers, noisy environments.\n",
    "  - Proper nouns and specific names are often misrecognized.\n",
    "  \n",
    "- **Voice Activity Detection (VAD)**: Recognizes when someone is speaking but doesn’t transcribe. Useful for detecting when a robot should start listening.\n",
    "\n",
    "- **Artificial Languages**:\n",
    "  - **Engineered Languages**: Designed for **logic, philosophy, or linguistics** experiments (e.g., **Loglan**, **ROILA**).\n",
    "    - **ROILA**: Developed for **HRI** to improve **speech-recognition accuracy**. Words are designed to sound distinct for better recognition by robots. Like \"Go forward\" = **kanek koloke**, \"Go back\" = **kanek nole**.\n",
    "  - **Auxiliary Languages**: Created to **bridge natural languages** or serve as an **international medium** (e.g., **Esperanto**).\n",
    "  - **Artistic Languages**: Made for **fictional worlds** (e.g., **Klingon**, **Elfish**, **Dothraki**).\n",
    "\n",
    "---\n",
    "\n",
    "### **7.3 Interacting Using Language**\n",
    "\n",
    "- **Speech Recognition ≠ Understanding**:\n",
    "  - ASR transcribes words but does not \"understand\" meaning.\n",
    "\n",
    "- **Extracting Meaning**:\n",
    "  - **Sentiment Analysis**: Determines the emotional tone of spoken text (positive or negative).\n",
    "  - **Intent Recognition**: Identifying the user’s goals or commands from speech (e.g., setting reminders).\n",
    "\n",
    "- **Chatbots in HRI**:\n",
    "  - Two types:\n",
    "    1. **Goal-specific chatbots**: Limited to specific domains (e.g., customer service).\n",
    "    2. **General-purpose chatbots**: Can respond to a wide range of topics using LLMs.\n",
    "  - Issues with chatbots: Sometimes they learn inappropriate responses (e.g., Microsoft’s Tay).\n",
    "  - **Multimodal Input**: Robots, unlike chatbots, use multiple sensors (vision, sound) to process more than just text.\n",
    "\n",
    "#### **7.4 Dialogue Management in HRI**\n",
    "\n",
    "- **Dialogue Management (DM)**:\n",
    "  - Manages back-and-forth conversations between users and robots.\n",
    "  - Can be:\n",
    "    - **System-initiative**: The robot leads the conversation.\n",
    "    - **User-initiative**: The user leads.\n",
    "    - **Mixed-initiative**: Both take turns leading.\n",
    "  \n",
    "- **Finite State Machines (FSM)**:\n",
    "  - Basic DM structures that follow linear dialogue flows.\n",
    "  \n",
    "- **Event-based DMs**:\n",
    "  - More advanced, allowing for dynamic conversation changes based on user input (e.g., interruptions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7.5 Speech Production**\n",
    "\n",
    "- Older Text-to-Speech (TTS) systems (e.g., Stephen Hawking’s speech synthesizer) were basic and unnatural. Modern systems (like **WaveNet**) are much more human-like.\n",
    "  \n",
    "- **Parametric TTS**: Uses a model of speech generation, flexible but less natural.\n",
    "  \n",
    "- **Neural Vocoders**: Deep learning-based systems trained on large speech datasets. They produce highly natural speech.\n",
    "  \n",
    "- **TTS Challenges**:\n",
    "  - **Emotion in speech**: Current models don't modulate emotion well.\n",
    "  - TTS sounds more like text being read than a natural conversation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
