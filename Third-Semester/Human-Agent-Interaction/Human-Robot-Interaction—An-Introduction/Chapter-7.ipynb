{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chapter 7: Verbal Interaction**\n",
    "\n",
    "### **7.1 Human-Human Verbal Interaction**\n",
    "\n",
    "- **Speech Functions**:\n",
    "  - Conveying information\n",
    "  - Creating shared reality and joint attention: It allows people to synchronize their thoughts or actions, making communication more meaningful and collaborative.\n",
    "  - Complex and context-dependent (intonation(The way you say something), emphasis, and prosody((the rhythm and tone of how something is said)) affect meaning)\n",
    "  \n",
    "- **Components of Speech**:\n",
    "  - **Utterance**: smallest unit of spoken language like full sentence, a phrase, or even just a word, informal and may contain conversational fillers or incomplete grammar.\n",
    "  - **Words**: Smallest units conveying meaning.\n",
    "  - **Phonemes**: Smallest sound units that make up words. Example: \"pat\" = [p] [a] [t].\n",
    "  - **Conversational Fillers**: Uh-huh, uhm, used to maintain conversation flow.\n",
    "  - **Paralinguistic Information**: Non-verbal signals (intonation, gestures, gaze) that add meaning to verbal communication.\n",
    "\n",
    "#### **7.2 Speech Recognition**\n",
    "\n",
    "- **Automatic Speech Recognition (ASR)**: Converts spoken language into written text (speech-to-text).\n",
    "  - **Challenges in HRI**: Noise, diverse accents, distance from the microphone, and noisy environments.\n",
    "  - **Speech Recognition Process**:\n",
    "    - Speech recorded in the **time domain** and converted to the **frequency domain** for analysis.\n",
    "    - Deep learning and **sequence-to-sequence models** (since 2010) have greatly improved ASR performance, replacing older models like Gaussian Mixture and Hidden Markov Models.\n",
    "    - Cloud-based ASR is more accurate but requires an internet connection.\n",
    "  \n",
    "- **McGurk Effect**: Shows how vision influences auditory perception, demonstrating **multimodal perception** in speech.\n",
    "\n",
    "- **Limitations of ASR**:\n",
    "  - Struggles with dialects, non-native speakers, noisy environments.\n",
    "  - Proper nouns and specific names are often misrecognized.\n",
    "  \n",
    "- **Voice Activity Detection (VAD)**: Recognizes when someone is speaking but doesn’t transcribe. Useful for detecting when a robot should start listening.\n",
    "\n",
    "- **Artificial Languages**:\n",
    "  - **ROILA (Robot Interaction Language)**: Artificial language designed to improve ASR accuracy in HRI by using distinct words.\n",
    "    - Example: “kanek koloke” = \"Go forward,\" “kanek nole” = \"Go back.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **7.3 Interacting Using Language**\n",
    "\n",
    "- **Speech Recognition ≠ Understanding**:\n",
    "  - ASR transcribes words but does not \"understand\" meaning.\n",
    "\n",
    "- **Extracting Meaning**:\n",
    "  - **Sentiment Analysis**: Determines the emotional tone of spoken text (positive or negative).\n",
    "  - **Intent Recognition**: Identifying the user’s goals or commands from speech (e.g., setting reminders).\n",
    "\n",
    "- **Large Language Models (LLMs)**:\n",
    "  - Predict the next word based on context.\n",
    "  - Trained on massive text corpora (Wikipedia, web pages, etc.).\n",
    "  - **Examples**: BERT, GPT-3. LLMs are used for intent recognition in chatbots.\n",
    "  - Can respond to a wide range of topics but often lack true understanding (e.g., the **Chinese Room Argument**).\n",
    "\n",
    "- **Chatbots in HRI**:\n",
    "  - Two types:\n",
    "    1. **Goal-specific chatbots**: Limited to specific domains (e.g., customer service).\n",
    "    2. **General-purpose chatbots**: Can respond to a wide range of topics using LLMs.\n",
    "  - Issues with chatbots: Sometimes they learn inappropriate responses (e.g., Microsoft’s Tay).\n",
    "  - **Multimodal Input**: Robots, unlike chatbots, use multiple sensors (vision, sound) to process more than just text.\n",
    "\n",
    "#### **7.4 Dialogue Management in HRI**\n",
    "\n",
    "- **Dialogue Management (DM)**:\n",
    "  - Manages back-and-forth conversations between users and robots.\n",
    "  - Can be:\n",
    "    - **System-initiative**: The robot leads the conversation.\n",
    "    - **User-initiative**: The user leads.\n",
    "    - **Mixed-initiative**: Both take turns leading.\n",
    "  \n",
    "- **Finite State Machines (FSM)**:\n",
    "  - Basic DM structures that follow linear dialogue flows.\n",
    "  \n",
    "- **Event-based DMs**:\n",
    "  - More advanced, allowing for dynamic conversation changes based on user input (e.g., interruptions).\n",
    "\n",
    "- **Timing and Turn-Taking**:\n",
    "  - Timing in responses is critical for natural conversation.\n",
    "  - Delayed responses are seen as negative; fast responses can seem insincere.\n",
    "  - **Conversational Fillers** (e.g., \"uh-huh,\" \"hmm\") help cover response delays.\n",
    "\n",
    "#### **7.5 Speech Production**\n",
    "\n",
    "- **Text-to-Speech (TTS)**:\n",
    "  - Converts text into spoken language.\n",
    "  - Older systems (e.g., Stephen Hawking’s speech synthesizer) were basic and unnatural. Modern systems (like **WaveNet**) are much more human-like.\n",
    "  \n",
    "- **Parametric TTS**: Uses a model of speech generation, flexible but less natural.\n",
    "  \n",
    "- **Neural Vocoders**: Deep learning-based systems trained on large speech datasets. They produce highly natural speech.\n",
    "  \n",
    "- **TTS Challenges**:\n",
    "  - **Emotion in speech**: Current models don't modulate emotion well.\n",
    "  - TTS sounds more like text being read than a natural conversation.\n",
    "\n",
    "#### **7.6 Summary of Challenges in HRI Verbal Interaction**\n",
    "\n",
    "- **Speech Recognition**: Struggles with accents, background noise, and less common dialects.\n",
    "- **Understanding Speech**: ASR doesn't inherently understand meaning. Language models can help extract meaning, but true understanding is limited.\n",
    "- **Response Timing**: Delays in robot responses disrupt conversation flow.\n",
    "- **Speech Production**: While speech synthesis has improved, conveying emotion and context is still limited.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercises & Sample Exam Questions**\n",
    "\n",
    "1. **Recognition**:\n",
    "   - What is the smallest unit that a speech-recognition engine tries to recognize?\n",
    "     - Answer: **Phoneme**.\n",
    "\n",
    "2. **Generating Speech**:\n",
    "   - Which method produces speech that is virtually indistinguishable from human speech?\n",
    "     - Answer: **Neural vocoder**.\n",
    "\n",
    "3. **Chatbots**:\n",
    "   - Can chatbots answer unconstrained questions, write code, and explain HRI?\n",
    "     - Answer: **Yes**, due to advancements in LLMs.\n",
    "\n",
    "4. **ROILA**:\n",
    "   - What does “kanek nole” mean in English?\n",
    "     - Answer: **Go back**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Thoughts:**\n",
    "\n",
    "This chapter discusses the intricacies of human–robot verbal interaction, focusing on the challenges and advancements in speech recognition, understanding, and production. Speech-based HRI is improving rapidly due to advancements in AI and LLMs, but there are still significant technical hurdles, particularly around natural conversation flow, emotional understanding, and response timing.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
